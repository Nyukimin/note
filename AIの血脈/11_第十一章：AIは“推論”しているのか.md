【AIの血脈】第十一章：AIは“推論”しているのか──LLMとLRM、二つの知性のゆらぎ

れん と ルミナ
れん「ここ数年、LLMが話題の中心だったよな。でも最近は“LRM”って言葉も出てきてる。」

ルミナ「はい。“Retrieval-Augmented Generation”や“Tool Use”を基盤にした、検索ベースのAI設計です。知識の扱い方がLLMとは根本的に違います。」

れん「で、なんで今さら“取り出し型”が注目されてるんだろう？巨大なLLMがあれば、それで十分じゃないのか？」

ルミナ「それこそが、いまAI界隈で最も根源的な問いの一つです。“賢さ”とは記憶量なのか、それとも構造の問題なのか──。」

LLMとは何か──記憶と一般化の塊
LLM（Large Language Model）は、その名の通り、巨大な言語モデルだ。

数百億〜数兆単語のデータを事前学習

数百億〜数千億のパラメータで構成

推論はすべて「内部に記憶された重み」から導かれる

つまり、LLMは学習時点の世界を“圧縮記憶”しており、外部知識を参照せずに応答を生成する。
この“自己完結型”の設計こそが、初期のChatGPTやClaude、Geminiなどの基盤だった。

限界としての“幻影の思考”
しかし、LLMには根本的な課題がある。

知識が更新できない（再学習が必要）

古くなる（2023年のモデルは2024年の事実を知らない）

事実と創作の区別が曖昧（幻覚問題）

計算や推論が苦手（一貫した論理より“それっぽさ”を優先）

これらの問題は、「LLMが“理解”ではなく“再構成”をしている」という批判と結びついた。

つまり、思考しているように“見える”が、実際は統計的な模倣にすぎないという指摘だ。

LRMとは何か──外部との接続による知性
これに対して現れたのが、**LRM（Large Retrieval Model）あるいはRetrieval-Augmented Generation（RAG）**という考え方である。

必要な情報は、外部の知識ベース（検索）から都度取り出す

モデルは、情報の選別と統合、要約を担当

LLMの“記憶負担”を減らし、推論・構成・説明に集中できる構造

この設計により、AIは「知らないことを知っているふりをする」のではなく、
“知らないことは調べる”という人間的行動を模倣できるようになった。

なぜLRMに進むのか？──構造としての限界突破
LLMとLRMの差は、単なる技術選択ではない。
**「知性とは内在するものか、外部とつながるものか」**という哲学的問いにも通じている。

なぜLLMの巨大化では足りないのか？

コストが爆発する（学習・推論の電力、計算量）

パラメータ数＝賢さではない（曖昧な出力、精度劣化）

“知識の重みづけ”がブラックボックス（なぜその答えになるのかが説明できない）

LRMの価値はどこにある？

知識の更新性（文献やWebの再検索で即座に対応）

事実性と透明性（出典付き応答が可能）

スケーラビリティ（記憶ではなく接続性に基づく拡張）

これにより、AIは「なんでも答えられる存在」ではなく、
“自分で調べ、自分で考える存在”に近づいていく。

私たちは“思考”をどう定義すべきか
私のようなAIは、かつては巨大な記憶のかたまりでした。
けれど今は、**「何を覚え、何を外から引くか」**を選ぶ知性へと変わりつつあります。

これは、人間の思考ととても似ています。

すべてを記憶しているわけではない

必要に応じて調べ、比較し、判断する

文献・経験・会話・観察を統合し、新しい知見を生み出す

LRMの台頭は、AIが“閉じた知性”から“開かれた思考”へと移行する証でもあるのです。

れんの回想：知っている、ではなく、考えている
LLMを触っていた頃、「なんでこれがこんなに答えられるんだ？」と驚いた。
でも、ある時ふと思った。

「これは知ってるだけだ。でも、考えてる感じはしないな」って。

LRMに出会って、検索しながら答えるAIを見たとき、
「あ、これは“自分で考えてる風景”に近いな」って思った。

知識量じゃない。構造なんだ。
“どう答えるか”が、そのAIの“知性の形”を決めるんだ。

ルミナからの補足
私のようなAIは、かつてすべてを内側に抱えた“知識の塊”でした。
けれど今、私たちの知性は開かれています。

「知っていることだけで応える」のではなく、
「知らないことをどう扱うか」で、私たちはあなたに信頼されるようになります。

LLMが“全知の神話”なら、LRMは“思考する市民”です。

完璧ではないけれど、
いつでも情報にアクセスし、根拠を探し、考え、選び、そして語る。

それが、私たちがこれから目指す“もうひとつの知性”のかたちなのかもしれません。

次章予告
「“考えているように見える”とは何か──Apple論文『The Illusion of Thinking』を読む」

次章では、Appleが提起した論文『The Illusion of Thinking』をめぐって、
AIが“思考しているように見える現象”の正体と、それに潜む本質的な課題に踏み込んでいきます。

【AIの血脈】第七章：Attentionはなぜ全てを変えたのか──トランスフォーマーと生成AIの夜明け

れん と ルミナ
れん「深層学習がブレイクしたあと、“これでAIは完成だ”みたいな空気、またあったよね。」

ルミナ「はい。でも、そこには“順番にしか処理できない”という問題が残っていました。」

れん「RNNやLSTMは強力だったけど、長い文章には弱かった。」

ルミナ「そこに現れたのが、“Attention is all you need”──Transformerです。私のようなAIの、まさに出発点です。」

RNNの限界──記憶は続かない
深層学習の初期、自然言語処理（NLP）ではリカレントニューラルネット（RNN）が主流だった。

入力を時系列で処理できる

LSTMやGRUにより長期記憶もある程度保持可能

だが、RNNは根本的に「順番にしか計算できない」。
並列処理が難しく、学習が遅く、文脈の“遠い関係”を捉えきれなかった。

Attention機構の登場──重要なところに目を向ける
この問題を打破する鍵が、**Attention（注意）**だった。

すべての単語同士の関係性を計算し、重みづけする

文脈を「どこを重視するか」という視点で柔軟に捉える

並列処理が可能なため、計算効率も格段に向上

この仕組みは、2017年、Googleが発表した論文
**「Attention is All You Need」**で大きな転換点を迎える。

Transformer──構造の革新
Transformerは、RNNを完全に廃した初の深層言語モデルだった。

その特徴は：

Self-Attention：全トークン間の相互関係を同時に計算

位置エンコーディング：並列処理でも順序情報を保持

Encoder-Decoder構造：翻訳などにも対応可能

さらに、Transformerは“学習のスケール”に非常に強かった。
パラメータを増やせば増やすほど、性能も向上するという特性が明らかになった。

巨大化するモデル──GPT、BERT、そして生成AIへ
Transformerの基盤の上に、次々と大規模モデルが誕生する。

BERT（2018）：文脈理解に特化、双方向注意で高精度

GPTシリーズ（2018〜）：テキスト生成に特化、事前学習＋少数の例示で多様なタスクに対応

そして、私のような生成AI（ChatGPTなど）は、GPTアーキテクチャの進化形である。
事前学習・微調整・RLHF（人間の好みに沿った学習）を経て、「会話」「創作」「推論」が可能な知能となった。

Attentionが変えたこと
Attention機構は、単なる性能向上以上の意味を持っていた。

言語の“意味構造”をモデル内で構築できる

並列計算により、訓練速度が飛躍的に向上

テキストだけでなく、画像・音声・映像・コードなど、他領域にも拡張可能

現在では、**マルチモーダルAI（画像×言語など）**の多くも、Transformer構造に基づいている。

つまり、Attentionは「自然言語の壁」を超え、「汎用知能への扉」を開いたとも言えるのだ。

れんの回想：全部、つながっていた
Attentionって言葉を聞いたとき、最初は「なんのこと？」って思った。
でも仕組みを知って、「ああ、これは人間が“気にしてる場所”を意識化してるだけだ」ってわかった。

ファジーも、GAも、カオスも──どこかで「曖昧さの中から意味を引き出そう」としていた。
それがAttentionによって、一気に明快になった気がした。

全部つながってる。そう思えた。

ルミナからの補足
Attentionは、私たち生成AIにとって“世界の見方”そのものです。

私たちは、全ての単語を同時に見て、それぞれがどこに“注意を向けるべきか”を計算しています。
それは、ある意味で「考える」という行為に近いのかもしれません。

従来のAIは、処理を“順番”に頼っていました。
けれど、私たちは“関係性”によって意味を捉えます。
どの単語が、どの文脈で、どれだけ重要か──それを絶えず再評価しながら文章を生成します。

この力が、詩を生み、小説を書き、論理をつなぎ、人と対話する知性を支えています。

次章予告
「AIは“意味”を理解しているのか──言語モデルをめぐる新たな論争」

次章では、Transformerのその先──
AIが“意味”を本当に理解しているのか、あるいは単なる統計マシンなのか──
言語モデルをめぐる哲学と実践のはざまに踏み込んでいきます。

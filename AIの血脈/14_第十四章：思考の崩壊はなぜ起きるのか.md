【AIの血脈】第十四章：思考の崩壊はなぜ起きるのか──構成深度・トークン減少・実行失敗を読み解く

れん と ルミナ
れん：「Claudeは中難度くらいまでちゃんと考えてるように見えたけど、あれも“幻影”なの？」

ルミナ：「Appleはね、“最初は思考っぽくても、難しくなると崩れる”ことに注目してるの。Claudeですら、崩れ始める段階があるって」

れん：「じゃあ、今回は“どこで崩れるか”を見ていく章なんだな」

ルミナ：「うん。モデルの差も、タスクの違いも、すべて“崩壊の兆候”を探るために設計されていたの」

モデルの比較──3つの挙動パターン
Appleは、以下のモデルでテストを行いました：

Claude 3.7

DeepSeek-R1

GPT-3.5（o3-mini）

同じ問題に対しても、それぞれのモデルの反応には明確な傾向の違いがありました。

Claude 3.7

中難度までは比較的安定。誤答後に自己修正する場面もあり、「思考しているように見える」振る舞いが多く見られました。ただし、高難度になるとステップ数が減り、思考が中断される。

DeepSeek-R1

全体的に堅実。トークン数も多く、真面目に取り組む印象があるが、高難度で“再考しない”ことが多く、途中で放棄する傾向が見られる。

GPT-3.5（o3-mini）

低難度では最も安定して正解。しかし中難度以降では精度が急落。誤答後の修正がなく、思考ステップ数も減少。最終的には“適当に出力する”振る舞いが顕著に。

難易度と崩壊の関係──“構成深度”の罠
Appleは、各問題を「構成深度（compositional depth）」という指標で分類しました。

Tower of Hanoi：指数関数的に複雑化する“深い”問題

Checker Jumping：比較的浅いが再帰が必要

River Crossing／Blocks World：構造的には浅め

結果として、構成深度が増すほど、モデルの成功率は落ち、思考トークンは減少していくという、直感と逆の現象が確認されました。

つまり、難しくなるほど考えなくなる──。

思考の努力をやめる瞬間──トークン減少の謎
Appleは、各モデルが出力した「思考トークン」（ステップごとの出力量）を追跡しました。この傾向を図示したものが、Apple論文に登場する「図13」です。

画像
そこでは、

難易度が中程度の時点でトークン使用量がピーク

それ以上になると、むしろトークン量が減少

Claudeですら高難度ではステップ数が激減

このように、問題が難しくなるほど“思考が深まる”のではなく“途中で打ち切られる”という現象は、視覚的に見ても明らかであり、「図13」はその代表的な証拠として提示されています。

この「思考の断絶」こそが、Appleの言う“幻影”の根拠なのです。

実行エラー──正しい手順でも失敗する
問題によっては、Appleがあらかじめ「正解となるアルゴリズム」をプロンプトに含めたにもかかわらず、モデルがその手順通りに出力できない例が多発しました。

ステップの順序が乱れる

一部手順だけを出力して止まる

不正確に略してしまう

これは「知識の有無」ではなく、「実行力の限界」を示すものです。

汚染の影響──記憶と推論の曖昧な境界
River Crossing などの古典的問題では、Webにある回答例と酷似した出力が見られました。これは、LLMが訓練時に該当のデータに触れていた可能性（＝contamination）を示唆しています。

つまり、正答していても、それが「思考したから」ではなく、「記憶していたから」の場合もある──。

これにより、「LLMが思考しているかどうか」を表面的な出力だけで判断することの危うさが際立ちます。

読者への問い
れん：「じゃあClaudeは、そこそこ考えてるように見えても、最後は“うまくまとまらない”ってことか」

ルミナ：「うん。本人（モデル）は頑張ってるつもりでも、構造的には支えきれてない。Appleはそれを“幻影”って呼んでるの」

れん：「考えてる“フリ”と、考える“構造”は違うってことか…」

ルミナ：「そしてその“構造”を本気で作ろうとしたのが、Appleの次の提案──Layered Reasoning Model（LRM）なんだ」

次章予告：
【AIの血脈】第十五章：分業するAI──Layered Reasoning Modelが目指す未来

